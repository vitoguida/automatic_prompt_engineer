generation:
  num_subsamples: 2 # number of
  num_demos: 1 # few-shot demonstration in the generated prompt
  num_prompts_per_subsample: 4 # number of generated different prompts to test
  # if num_subsamples is 20 and num_prompts_per_subsample is 40,
  # then for each subsamples, 40 prompts are generated, with a total of 800.

  model:
    name: LocalLlama
    batch_size: 500
    model_config:
      model: meta-llama/Llama-3.2-3B-Instruct
      temperature: 0.9
      max_tokens: 500
      top_p: 0.9
      frequency_penalty: 0.0
      presence_penalty: 0.0
evaluation:
  method: exec_accuracy
  num_samples: 30
  num_few_shot: 5
  model:
    name: LocalLlama
    batch_size: 20
    model_config:
      model: meta-llama/Llama-3.2-3B-Instruct
      temperature: 0.7
      max_tokens: 500
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
demo:
  model:
    name: LocalLlama
    batch_size: 500
    model_config:
      model: meta-llama/Llama-3.2-3B-Instruct
      temperature: 0.7
      max_tokens: 500
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
