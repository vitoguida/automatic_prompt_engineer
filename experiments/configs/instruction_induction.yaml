generation:
  num_subsamples: 20 # number of
  num_demos: 10 # few-shot demonstration in the generated prompt
  num_prompts_per_subsample: 40 # number of generated different prompts to test
  # if num_prompts is 50 and num_subsamples is 5, then for each of the 5 subsamples, 10 prompts are generated and evaluated. ??
  model:
    name: LocalLlama
    batch_size: 500
    model_config:
      model: meta-llama/Llama-3.2-3B-Instruct
      temperature: 0.9
      max_tokens: 500
      top_p: 0.9
      frequency_penalty: 0.0
      presence_penalty: 0.0
evaluation:
  method: exec_accuracy
  num_samples: 30
  num_few_shot: 5
  model:
    name: LocalLlama
    batch_size: 20
    model_config:
      model: meta-llama/Llama-3.2-3B-Instruct
      temperature: 0.7
      max_tokens: 500
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
demo:
  model:
    name: LocalLlama
    batch_size: 500
    model_config:
      model: meta-llama/Llama-3.2-3B-Instruct
      temperature: 0.7
      max_tokens: 500
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
